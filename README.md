# **üìã Table des mati√®res**

√Ä propos du projet

Dataset

Architecture du projet

Technologies utilis√©es

Installation

Utilisation

M√©thodologie

R√©sultats

API REST

Analyse de sensibilit√©


# **üéØ √Ä propos du projet**
Ce projet vise √† construire un mod√®le de Machine Learning pour pr√©dire les frais d'assurance sant√© en fonction de caract√©ristiques personnelles et comportementales des assur√©s. L'objectif est d'am√©liorer l'efficacit√© et la rentabilit√© des compagnies d'assurance maladie en leur permettant d'√©valuer plus pr√©cis√©ment les risques et de tarifer leurs offres.
Probl√©matique
Comment construire un mod√®le d'apprentissage automatique pour am√©liorer l'efficacit√© et la rentabilit√© des compagnies d'assurance maladie ?
# **üìä Dataset**
L'ensemble de donn√©es contient 27 000 observations avec 7 variables :
VariableTypeDescriptionageQuantitative√Çge de l'assur√©sexQualitative binaireSexe (Male/Female)bmiQuantitativeIndice de Masse CorporellechildrenQuantitativeNombre d'enfants √† chargesmokerQualitative binaireStatut fumeur (Yes/No)regionQualitativeR√©gion de r√©sidence (4 modalit√©s)chargesQuantitativeFrais m√©dicaux (variable cible)
Source : Kaggle - Medical Insurance Price Prediction
# **üèóÔ∏è Architecture du projet**
üì¶ projet-assurance
‚îú‚îÄ‚îÄ üìÑ assurance.py          # Script principal
‚îú‚îÄ‚îÄ üìÑ application.py        # API FastAPI
‚îú‚îÄ‚îÄ üìÑ modele.pkl           # Mod√®le entra√Æn√© sauvegard√©
‚îú‚îÄ‚îÄ üìÑ campagne.csv         # Exemple de donn√©es de test
‚îú‚îÄ‚îÄ üìÅ static/
‚îÇ   ‚îî‚îÄ‚îÄ favicon.ico
‚îî‚îÄ‚îÄ üìÑ README.md
üõ†Ô∏è Technologies utilis√©es
Data Science & Machine Learning

Python 3.8+
pandas - Manipulation de donn√©es
numpy - Calculs num√©riques
scikit-learn - Mod√®le RandomForest et preprocessing
OpenTURNS - Analyse de sensibilit√© (indices de Sobol)

Visualisation

matplotlib - Graphiques statistiques
seaborn - Visualisations avanc√©es
yellowbrick - Diagnostics de r√©gression

D√©ploiement

FastAPI - API REST
uvicorn - Serveur ASGI
joblib - S√©rialisation du mod√®le
Pydantic - Validation des donn√©es

# **üì• Installation**
*Pr√©requis*
bashpython >= 3.8
pip >= 21.0
√âtapes d'installation

Cloner le repository

bashgit clone https://github.com/votre-username/projet-assurance.git
cd projet-assurance

Cr√©er un environnement virtuel

bashpython -m venv venv
venv\Scripts\activate  # Windows

*Installer les d√©pendances*

bashpip install pandas numpy matplotlib seaborn scikit-learn
pip install kagglehub joblib openturns yellowbrick
pip install fastapi uvicorn pydantic
# **üöÄ Utilisation**

1. Entra√Ænement du mod√®le
   
  bashpython assurance.py
  Le script effectue :

    ‚úÖ Chargement et exploration des donn√©es
    ‚úÖ Analyse exploratoire (EDA)
    ‚úÖ Pr√©traitement des donn√©es
    ‚úÖ Optimisation des hyperparam√®tres (GridSearchCV)
    ‚úÖ √âvaluation du mod√®le
    ‚úÖ Analyse de sensibilit√© (Sobol)
    ‚úÖ Sauvegarde du mod√®le (modele.pkl)

2. Lancement de l'API
   
  python application.py
  
  L'API sera accessible sur http://localhost:8000
  
# **Documentation interactive**

Swagger UI : http://localhost:8000/docs
ReDoc : http://localhost:8000/redoc

3. Exemple de pr√©diction
4. 
   "http://localhost:8000/deploiement/?age=24&sexe=Homme&bmi=23&children=2&smoker=Yes&region=Nord"



# Pr√©diction
prediction = model.predict(data)
print(f"Charges estim√©es : ${prediction[0]:,.2f}")
üî¨ M√©thodologie
1. Pr√©traitement des donn√©es

Encodage des variables cat√©gorielles (sexe, fumeur, r√©gion)
Standardisation des variables num√©riques
Pipeline scikit-learn pour automatisation

2. Mod√©lisation

Algorithme : RandomForestRegressor
Optimisation : GridSearchCV avec validation crois√©e (5-fold)
Hyperparam√®tres optimis√©s :

n_estimators : [80, 90, 100, 200]
max_depth : [5, 10, 15, 20, 25, 30, None]
min_samples_split : [2, 5, 10, 15, 20]
min_samples_leaf : [8, 10, 20, 25]
max_features : ['sqrt', 'log2', None]



3. √âvaluation

RMSE (Root Mean Squared Error)
R¬≤ (Coefficient de d√©termination)
MAPE (Mean Absolute Percentage Error)
Analyse des r√©sidus (Yellowbrick)
Courbes d'apprentissage

üìà R√©sultats
Les performances du mod√®le sont √©valu√©es sur l'ensemble de test (30% des donn√©es) :
M√©triques de performance :
‚îú‚îÄ‚îÄ RMSE : [√Ä remplir apr√®s ex√©cution]
‚îú‚îÄ‚îÄ R¬≤ : [√Ä remplir apr√®s ex√©cution]
‚îî‚îÄ‚îÄ MAPE : [√Ä remplir apr√®s ex√©cution]
Variables les plus importantes
L'analyse montre que les facteurs cl√©s influen√ßant les charges sont :

Statut fumeur (smoker)
√Çge (age)
IMC (bmi)

üåê API REST
Endpoints disponibles
1. Information g√©n√©rale
httpGET /
Retourne les informations sur l'API.
2. Pr√©diction
httpGET /deploiement/
Param√®tres :

age (int) : √Çge de l'assur√©
sexe (enum) : "Homme" ou "Femme"
bmi (float) : Indice de Masse Corporelle
children (int) : Nombre d'enfants
smoker (enum) : "Yes" ou "Non"
region (enum) : "Nord", "Sud", "Est" ou "Ouest"

R√©ponse :
json{
  "Sa charge est de ": 12345.67
}
Exemple avec Postman
GET http://localhost:8000/deploiement/?age=35&sexe=Femme&bmi=28.5&children=1&smoker=Non&region=Sud
üîç Analyse de sensibilit√©
Le projet inclut une analyse de sensibilit√© compl√®te utilisant les indices de Sobol via OpenTURNS.
Objectif
Identifier quelles variables ont le plus d'impact sur les pr√©dictions du mod√®le.
Indices calcul√©s

Indices de premier ordre : Effet direct de chaque variable
Indices totaux : Effet direct + interactions

Interpr√©tation
Plus l'indice de Sobol est √©lev√©, plus la variable est influente dans les pr√©dictions du mod√®le.
üìù Statistiques descriptives
Le code g√©n√®re des analyses statistiques compl√®tes :

Distribution des variables par cat√©gorie
Corr√©lations entre variables
Boxplots et scatter plots
Graphiques circulaires pour les variables cat√©gorielles

























